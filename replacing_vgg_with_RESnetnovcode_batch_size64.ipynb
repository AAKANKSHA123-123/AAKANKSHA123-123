{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAKANKSHA123-123/AAKANKSHA123-123/blob/main/replacing_vgg_with_RESnetnovcode_batch_size64.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OROjo4JQbxax"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmvspTf4aPO"
      },
      "source": [
        "**Introduction and data collection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYn2X0l74fuY"
      },
      "outputs": [],
      "source": [
        "#importing packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4fFA5_xI_9-",
        "outputId": "3180e280-9fcc-4c9a-8dc5-28558e37399a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done imports\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# We'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import operator\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from tensorflow import reshape\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Embedding, LSTM, Activation,ZeroPadding1D,Conv1D\n",
        "\n",
        "print(\"Done imports\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7C8C7SWk6GV",
        "outputId": "8ffdd8c1-e153-4af1-ae1e-ce28d989f2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GueLPOzUT3L"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKs2drkS4ul_"
      },
      "source": [
        "**Mounting drive for saving data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtxNqaxh4r3e",
        "outputId": "b5e5f640-da4a-4da8-bee9-d0bfb426a26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/', force_remount=True) #mouting the drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YL_u-Fw5ayf"
      },
      "source": [
        "**Downloading COCO images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VHRS2f57TFa"
      },
      "source": [
        "**Downloading VQA Questions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvYkQ6aoJC77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e58a1f-904f-4f0a-8813-68268d3a9204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 6s 0us/step\n",
            "252887040/252872794 [==============================] - 6s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "12904341504/13510573713 [===========================>..] - ETA: 12s"
          ]
        }
      ],
      "source": [
        "annotation_zip = tf.keras.utils.get_file('captions.zip',cache_subdir=os.path.abspath('.'),origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',extract = True)\n",
        "\n",
        "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "\n",
        "name_of_zip = 'train2014.zip'\n",
        "if not os.path.exists(os.path.abspath('.')+'/' + name_of_zip):\n",
        "  image_zip = tf.keras.utils.get_file(name_of_zip,cache_subdir=os.path.abspath('.'),origin='http://images.cocodataset.org/zips/train2014.zip',extract=True)\n",
        "  PATH = os.path.dirname(image_zip)+'/train2014'\n",
        "else:\n",
        "  PATH = os.path.abspath('.')+'/train2014'  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BreD8lRjjZ2-"
      },
      "outputs": [],
      "source": [
        "print(PATH)\n",
        "#!wget https://s3.amazonaws.com/cvmlp/vqs/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
        "#!unzip -a v2_Questions_Train_mscoco.zip\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip\n",
        "!unzip -a v2_Annotations_Train_mscoco.zip\n",
        "!wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip\n",
        "!unzip -a v2_Questions_Train_mscoco.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMPsiQXt7d8W"
      },
      "outputs": [],
      "source": [
        "# read the json file\n",
        "question_file_path = 'v2_OpenEnded_mscoco_train2014_questions.json'\n",
        "with open(question_file_path, 'r') as f:\n",
        "    questions = json.load(f)\n",
        "\n",
        "print(\"Total Number Questions is : \",len(questions['questions']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8Bg2TP5mDZs"
      },
      "source": [
        "**Added code from separate image model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnszjnLpOao_"
      },
      "outputs": [],
      "source": [
        "#Storing the captions and image file name in vectors\n",
        "import collections\n",
        "import operator\n",
        "\n",
        "annotation_file = 'v2_mscoco_train2014_annotations.json'\n",
        "\n",
        "with open(annotation_file,'r') as f:\n",
        "  annotations = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "all_answers = []\n",
        "all_answers_qids = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "  #print(annot)\n",
        "  ans_dic = collections.defaultdict(int)\n",
        "  for each in annot['answers']:\n",
        "    diffans = each['answer']\n",
        "    if diffans in ans_dic:\n",
        "      #print(each['answer_confidence'])\n",
        "      if each['answer_confidence']=='yes':\n",
        "        ans_dic[diffans]+=4\n",
        "      if each['answer_confidence']=='maybe':\n",
        "        ans_dic[diffans]+=2\n",
        "      if each['answer_confidence']=='no':\n",
        "        ans_dic[diffans]+=1\n",
        "    else:\n",
        "      if each['answer_confidence']=='yes':\n",
        "        ans_dic[diffans]+=4\n",
        "      if each['answer_confidence']=='maybe':\n",
        "        ans_dic[diffans]+=2\n",
        "      if each['answer_confidence']=='no':\n",
        "        ans_dic[diffans]+=1\n",
        "\n",
        "  #print(ans_dic)\n",
        "  most_fav = max(ans_dic.items(),key=operator.itemgetter(1))[0]\n",
        "  #print(most_fav)\n",
        "  caption = '<start>' + most_fav + '<end>'  #each['answer']\n",
        "\n",
        "  image_id = annot['image_id']\n",
        "  question_id = annot['question_id']\n",
        "  full_coco_image_path = PATH  + '/COCO_train2014_' + '%012d.jpg' %(image_id)\n",
        "\n",
        "  all_img_name_vector.append(full_coco_image_path)\n",
        "  all_answers.append(caption)\n",
        "  all_answers_qids.append(question_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcbRXitEmRPg"
      },
      "outputs": [],
      "source": [
        "all_questions = []\n",
        "question_ids = []\n",
        "all_img_name_vector_2 = []\n",
        "\n",
        "\n",
        "for annot in questions['questions']:\n",
        "  caption = '<start>' + annot['question'] + '<end>'\n",
        "  image_id = annot['image_id']\n",
        "  full_coco_image_path = PATH + '/COCO_train2014_' + '%012d.jpg' %(image_id)\n",
        "  \n",
        "  all_img_name_vector_2.append(full_coco_image_path)\n",
        "  all_questions.append(caption)\n",
        "  #print(all_questions)\n",
        "  question_ids.append(annot['question_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QopPOmFB7n1I"
      },
      "outputs": [],
      "source": [
        "print(questions['questions'][np.random.randint(0,443757)])#new ms coco code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_4LKh0PsguW"
      },
      "outputs": [],
      "source": [
        "#Taken from separate image model\n",
        "print(len(all_img_name_vector),len(all_answers),len(all_answers_qids))\n",
        "print(all_img_name_vector[10:15],all_answers[10:15],all_answers_qids[10:15])\n",
        "print(len(all_img_name_vector_2),len(all_questions),len(question_ids))\n",
        "print(all_img_name_vector_2[10:15],all_questions[10:15],question_ids[10:15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo7MikMhtobE"
      },
      "source": [
        "**Shuffling all questions and answers(taken from separate image model)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2qyqQIttbRI"
      },
      "outputs": [],
      "source": [
        "train_answers,train_questions,img_name_vector = shuffle(all_answers,all_questions,all_img_name_vector,random_state=1)\n",
        "#train_answers,train_questions,img_name_vector = (all_answers,all_questions,all_img_name_vector)\n",
        "num_examples=50\n",
        "\n",
        "train_answers = train_answers[:num_examples]\n",
        "train_questions = train_questions[:num_examples]\n",
        "img_name_vector =img_name_vector[:num_examples]\n",
        "\n",
        "print(img_name_vector[0],train_questions[0],train_answers[0])\n",
        "print(len(img_name_vector),len(train_questions),len(train_answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc9SVSxG7qsA"
      },
      "source": [
        "**Downloading VQA Annotations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRpom8Gw8EOF"
      },
      "outputs": [],
      "source": [
        "annotation_file_path = 'v2_mscoco_train2014_annotations.json'\n",
        "with open(annotation_file_path, 'r') as f:\n",
        "    annotations = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1fz3ESM8Ip6"
      },
      "outputs": [],
      "source": [
        "annotations['annotations'][np.random.randint(0,443757)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jpoHHwSAytZ"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40QtbiNm8mUH"
      },
      "source": [
        "**Data preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNL7NDrY8xVK"
      },
      "outputs": [],
      "source": [
        "#importing packages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from wordcloud import WordCloud\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrEB5GER84Nz"
      },
      "source": [
        "**Variables**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvKwZK3f9C-q"
      },
      "source": [
        "**DATA TRANSFORMATION**\n",
        "LOADING QUESTIONS AND ANSWERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HPtVxpl9LZl"
      },
      "outputs": [],
      "source": [
        "with open(question_file_path, 'r') as f:\n",
        "    questions = json.load(f)\n",
        "    questions = questions[\"questions\"]\n",
        "\n",
        "with open(annotation_file_path, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "    annotations = annotations[\"annotations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTATT8Ek9SGu"
      },
      "outputs": [],
      "source": [
        "print(\"Total Number Questions is : \",len(questions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owLMwP449VlR"
      },
      "outputs": [],
      "source": [
        "questions_df = pd.DataFrame(questions).sample(n=250000)\n",
        "#questions_df = pd.DataFrame(questions)\n",
        "#ques_samp = questions_df.sample(n=100)\n",
        "#ques_samp.head(5)\n",
        "\n",
        "questions_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhjoeUHX9eDC"
      },
      "outputs": [],
      "source": [
        "annotations_df = pd.DataFrame(annotations).sample(n=250000)\n",
        "#annotations_df = pd.DataFrame(annotations)\n",
        "#annot_samp = annotations_df.sample(n=100)\n",
        "#annot_samp.head(5)\n",
        "annotations_df.head(5)\n",
        "print(len(annotations_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5YVRnoT9gWJ"
      },
      "source": [
        "**MERGING QUESTIONS AND ANNOTATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE4B2jA-9li7"
      },
      "outputs": [],
      "source": [
        "data = pd.merge(questions_df,annotations_df,  how='inner', left_on=['image_id','question_id'], right_on = ['image_id','question_id'])\n",
        "#data_samp = pd.merge(ques_samp,annot_samp,  how='inner', left_on=['image_id','question_id'], right_on = ['image_id','question_id'])\n",
        "print(len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4w7Sp4W9sly"
      },
      "outputs": [],
      "source": [
        "data.head(5)\n",
        "#data_samp.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W4R3ptV_AVh"
      },
      "outputs": [],
      "source": [
        "#currentDirectory = \"/content/drive/My Drive/pcase_study_2/\"\n",
        "#os.chdir(currentDirectory)\n",
        "#currentDirectory = \"\"\n",
        "#dataDirectory = currentDirectory + \"data/\"\n",
        "#imageDirectory = dataDirectory + \"train2014/\"\n",
        "imageDirectory = '/content/train2014'\n",
        "#question_file_path = dataDirectory + 'v2_OpenEnded_mscoco_train2014_questions.json'\n",
        "question_file_path = '/content/v2_OpenEnded_mscoco_train2014_questions.json'\n",
        "#annotation_file_path = dataDirectory + 'v2_mscoco_train2014_annotations.json'\n",
        "annotation_file_path = '/content/v2_mscoco_train2014_annotations.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh6QGTaA9yr3"
      },
      "outputs": [],
      "source": [
        "imageDirectory = '/content/train2014'\n",
        "os.chdir(imageDirectory)\n",
        "imageDirectory = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u3eHGCq9402"
      },
      "outputs": [],
      "source": [
        "index = np.random.randint(0,len(data))#263115\n",
        "\n",
        "img_path =  imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (data['image_id'][index])\n",
        "img=mpimg.imread(img_path)\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "print(\"*\"*50)\n",
        "print(\"Question : \" ,data['question'][index])\n",
        "print(\"*\"*50)\n",
        "print(\"Answer : \", data['multiple_choice_answer'][index])\n",
        "\n",
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk_TwQnH97wI"
      },
      "source": [
        "**IMAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz9Rv0Im9-Ed"
      },
      "outputs": [],
      "source": [
        "aggregations = {'question': 'count'}\n",
        "temp = pd.DataFrame(data.groupby(['image_id'],as_index=False).agg(aggregations))\n",
        "#temp.set_index('image_id', inplace=True)\n",
        "\n",
        "print(temp)\n",
        "print(len(temp))\n",
        "num_of_ques_in_image = temp['question'].values\n",
        "print(\"Max number of questions on a image\",max(num_of_ques_in_image))\n",
        "print(\"Min number of questions on a image\",min(num_of_ques_in_image))\n",
        "print(\"Mean of questions on a image\",np.mean(num_of_ques_in_image))\n",
        "\n",
        "ax = sns.boxplot(y = 'question', data = temp) \n",
        "plt.title(\"boxplot of Number of questions on a iage\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3aJnQcG-FC7"
      },
      "source": [
        "**IMAGES THAT HAVE MAXIMUM NUMBER OF QUESTIONS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku8FgAhf-PLJ"
      },
      "source": [
        "**Duplicate questions on same image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXROCub-TDN"
      },
      "outputs": [],
      "source": [
        "aggregations = {'question_id':'count', 'multiple_choice_answer': lambda x: \" || \".join(x)}\n",
        "temp = pd.DataFrame(data.groupby(['image_id','question'],as_index=False).agg(aggregations)).rename(columns={'question_id':'count'})\n",
        "temp = temp[temp['count']>1]\n",
        "temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmlV3D_L-jsA"
      },
      "source": [
        "**Question type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrohE_f5-MJO"
      },
      "outputs": [],
      "source": [
        "data.question_type.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xelvejb9-BKw"
      },
      "outputs": [],
      "source": [
        "print(\"Number of unique Question type in dataset : \",len(data.question_type.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkL3IuvI-w6J"
      },
      "outputs": [],
      "source": [
        "def getFrequnctDict(data,column,isJoin=False):\n",
        "    column_frequency = {}\n",
        "\n",
        "    for _row in data[column]:\n",
        "        if isJoin:\n",
        "            _row = \"_\".join(_row.split())\n",
        "        if(column_frequency.get(_row,-1) > 0):\n",
        "            column_frequency[_row] += 1\n",
        "        else:\n",
        "            column_frequency[_row] = 1\n",
        "\n",
        "    return column_frequency\n",
        "\n",
        "def lineChart(data,column,top=20,isJoin=False):\n",
        "    column_frequncy = getFrequnctDict(data,column,isJoin)\n",
        "    sort_column_frequncy = sorted(list(column_frequncy.items()),key = lambda x: x[1],reverse=True)\n",
        "    total_samples =  len(data)\n",
        "\n",
        "    plt.plot([x[1]for x in sort_column_frequncy[:top]])\n",
        "    i=np.arange(top)\n",
        "    plt.title(\"Frequency of top \" + str(top) + \" \" + column )\n",
        "    plt.xlabel(\"Tags\")\n",
        "    plt.ylabel(\"Counts\")\n",
        "    plt.xticks(i,[x[0] for x in sort_column_frequncy[:top]])\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "    return sort_column_frequncy\n",
        "\n",
        "def plotWordCloud(data,column,isJoin=False):\n",
        "    column_frequncy = getFrequnctDict(data,column,isJoin)\n",
        "    #https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
        "    wordcloud = WordCloud(width = 800, height = 800, \n",
        "                    background_color ='white', \n",
        "                    stopwords = None, \n",
        "                    min_font_size = 10).generate_from_frequencies(column_frequncy)\n",
        "    # plot the WordCloud image     \n",
        "    plt.figure(figsize = (8, 8), facecolor = None) \n",
        "    plt.imshow(wordcloud) \n",
        "    plt.axis(\"off\") \n",
        "    plt.tight_layout(pad = 0) \n",
        "    plt.title(\"WordCloud on \"+ column)  \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUqFYfNA-01B"
      },
      "outputs": [],
      "source": [
        "plotWordCloud(data, 'question_type')\n",
        "question_type_frequncy = lineChart(data, 'question_type', top = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_JsyUY-8Kr"
      },
      "outputs": [],
      "source": [
        "for _type,_count in question_type_frequncy[:10]:\n",
        "    print(\"Percentage of '\" + _type + \"' Type of Questions in Dataset is \", str(100*_count/len(data))) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N4YrPDI-_aH"
      },
      "source": [
        "**ANSWERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4CsQ8KT_Cb4"
      },
      "outputs": [],
      "source": [
        "data['answer_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltZWucWC_HeY"
      },
      "outputs": [],
      "source": [
        "answer_type_frequncy = lineChart(data, 'answer_type', top = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P-TIhuV_MGp"
      },
      "outputs": [],
      "source": [
        "for _type,_count in answer_type_frequncy:\n",
        "    print(\"Percentage of '\" + _type + \"' Type of Answers in Dataset is \", str(100*_count/len(data)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlkBxnS7_NGg"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data[\"multiple_choice_answer\"].apply(lambda x: len(x.split())).values)\n",
        "plt.title(\"Number of words in Answers vs Distrubution\")\n",
        "plt.xlabel(\"Number of words in Answers\")\n",
        "plt.ylabel(\"Distrubution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh-l0a70_TDx"
      },
      "source": [
        "**QUESTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maSkjYJh_QoD"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data[\"question\"].apply(lambda x: len(x.split())).values)\n",
        "plt.title(\"Length of the questions vs Distrubution\")\n",
        "plt.xlabel(\"Length of the questions\")\n",
        "plt.ylabel(\"Distrubution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYIuwthA_aok"
      },
      "source": [
        "**QUESTION TYPE AND ANSWER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dgkIIrE_e2l"
      },
      "outputs": [],
      "source": [
        "temp = data\n",
        "top_question = [x[0] for x in question_type_frequncy[:50]]\n",
        "print(top_question)\n",
        "top_aswers = [x[0] for x in answer_type_frequncy[:50]]\n",
        "print(top_aswers)\n",
        "temp = data[(data['question_type'].isin(top_question) | data['multiple_choice_answer'].isin(top_aswers))]\n",
        "\n",
        "aggregations = {'question': 'count'}\n",
        "temp = pd.DataFrame(temp.groupby(['question_type','multiple_choice_answer'],as_index= False).agg(aggregations))\n",
        "temp = temp[temp['question']>=10]\n",
        "temp = temp.pivot(index='question_type', columns='multiple_choice_answer', values='question')\n",
        "#print(temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUAW_udt_qqF"
      },
      "source": [
        "**QUESTION TYPE VS ANSWER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzxkOQvq_2SA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(80,30))\n",
        "fig.tight_layout() \n",
        "count = 1\n",
        "colorCodes = [ \"#E18719\", \"#287819\", \"#2D6EAA\", \"#E6AF23\", \"#666666\",\"#724D8D\", \"#EAAB5E\", \"#73A769\",\"#93785F\",\n",
        "              \"#C97B7B\", \"#81A8CC\", \"#EDC765\", \"#858585\",\"#957AA9\", \"#F3CFA3\",\"#B4D0AF\", \"#BEADA0\", \"#E4BDBD\", \n",
        "              \"#ABC5DD\", \"#F4DB9C\", \"#A3A3A3\"]\n",
        "\n",
        "for _type,_ in question_type_frequncy[:12]:\n",
        "\n",
        "    percentage = str(round((len(data[data['question_type']==_type])/len(data))*100,1))+'%'\n",
        "\n",
        "    plt.subplot(4, 3, count)\n",
        "    temp = data[data['question_type']==_type]\n",
        "    ax = temp['multiple_choice_answer'].value_counts()[:10][::-1].plot(kind='barh', figsize=(20,15),color=colorCodes[count-1], fontsize=13)\n",
        "    ax.set_alpha(0.8)   \n",
        "    ax.set_title(\"Question Type:  '\" + _type + \"' (\" + percentage + \") vs Answer\" , fontsize=18)\n",
        "    ax.set_ylabel(\"Answers\", fontsize=18)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "\n",
        "    for i in ax.patches:\n",
        "        ax.text(i.get_width()/2, i.get_y(), str(round((i.get_width()/len(temp))*100, 2))+'%' + \"(\" +\n",
        "                str(round((i.get_width()/len(data))*100, 2))+'%' +\")\", fontsize=10,color='black')\n",
        "        \n",
        "    count += 1\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9fUZO_u_5vD"
      },
      "source": [
        "**ANSWERS VS QUESTION TYPE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT91jjKQ_9GR"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "fig.tight_layout() \n",
        "count = 1\n",
        "\n",
        "colorCodes = [ \"#E18719\", \"#287819\", \"#2D6EAA\", \"#E6AF23\", \"#666666\",\"#724D8D\", \"#EAAB5E\", \"#73A769\",\"#93785F\",\n",
        "              \"#C97B7B\", \"#81A8CC\", \"#EDC765\", \"#858585\",\"#957AA9\", \"#F3CFA3\",\"#B4D0AF\", \"#BEADA0\", \"#E4BDBD\", \n",
        "              \"#ABC5DD\", \"#F4DB9C\", \"#A3A3A3\"]\n",
        "\n",
        "answer_frequncy = sorted(list(getFrequnctDict(data,'multiple_choice_answer').items()),key = lambda x: x[1],reverse=True)\n",
        "\n",
        "for _type,_ in answer_frequncy[:12]:\n",
        "\n",
        "    percentage = str(round((len(data[data['multiple_choice_answer']==_type])/len(data))*100,1))+'%'\n",
        "\n",
        "    plt.subplot(4, 3, count)\n",
        "    temp = data[data['multiple_choice_answer']==_type]\n",
        "    ax = temp['question_type'].value_counts()[:10][::-1].plot(kind='barh', figsize=(20,15),color=colorCodes[count-1], fontsize=13)\n",
        "    ax.set_alpha(0.8)   \n",
        "    ax.set_title(\"Answer: '\" + _type + \"' (\" + percentage + \") vs Question Type\" , fontsize=18)\n",
        "    ax.set_ylabel(\"Question Type\", fontsize=18)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "    for i in ax.patches:\n",
        "        ax.text(i.get_width()/2, i.get_y(), str(round((i.get_width()/len(temp))*100, 2))+'%' + \"(\" +\n",
        "                str(round((i.get_width()/len(data))*100, 2))+'%' +\")\", fontsize=14,color='black')\n",
        "        \n",
        "    count += 1\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI8blK94AE5W"
      },
      "source": [
        "**CHECKING IF ACTUAL ANSWER IS SAME AS PERSONS ANSWERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTKhrUGSBILb"
      },
      "outputs": [],
      "source": [
        "def getPeopleAnswer(answers):\n",
        "    answers_dict = {}\n",
        "    score_dict = { 'yes' : 3, 'maybe' : 2, 'no' : 1 }\n",
        "    for _answer in answers:\n",
        "        score = score_dict[_answer['answer_confidence']]\n",
        "        if answers_dict.get(_answer['answer'],-1) != -1 :\n",
        "            answers_dict[_answer['answer']] += score\n",
        "        else:\n",
        "            answers_dict[_answer['answer']] = score\n",
        "\n",
        "    return sorted(list(answers_dict.items()),key = lambda x: x[1],reverse=True)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q_aDquyBMSx"
      },
      "outputs": [],
      "source": [
        "#data['derived_answer'] =  data[\"answers\"].apply(lambda x: getPeopleAnswer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYYW8CqJBPcK"
      },
      "outputs": [],
      "source": [
        "#data[ data['derived_answer'] != data['multiple_choice_answer']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj8zaMKqdTu5"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-z1eeB8BSen"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataDDirectory ='/content/drive/MyDrive/newresults'\n",
        "os.chdir(dataDDirectory)\n",
        "!pwd\n",
        "data.to_csv(dataDDirectory + 'data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPMndP3tda-p"
      },
      "outputs": [],
      "source": [
        "os.listdir(dataDDirectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ1dC70wBcK6"
      },
      "source": [
        "**Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOlGteisBfMz"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import random as rn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.regularizers import l1,l2\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "from sklearn.utils import shuffle\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8rqXlZBqKg"
      },
      "source": [
        "**Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTVHlcuZBskq"
      },
      "outputs": [],
      "source": [
        "currentDirectory = \"/content/drive/MyDrive/newresults/\"\n",
        "os.chdir(currentDirectory)\n",
        "currentDirectory = \"\"\n",
        "dataDirectory = currentDirectory + \"data/\"\n",
        "#imageDirectory = dataDirectory + \"train2014/\"\n",
        "imageDirectory = '/content/train2014/'\n",
        "imageNumpyDirectory = dataDirectory + \"train2014_Numpy\" + ''\n",
        "\n",
        "modelsDirectory = currentDirectory + \"Models/\"\n",
        "\n",
        "\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "#BATCH_SIZE = 1\n",
        "BUFFER_SIZE = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wt8d3JiBzT3"
      },
      "outputs": [],
      "source": [
        "#len(os.listdir(imageNumpyDirectory))\n",
        "data = pd.read_csv(dataDDirectory + 'data.csv')\n",
        "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym4rXfDHB2jb"
      },
      "source": [
        "**Creating answer vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OWSlh9eB0le"
      },
      "outputs": [],
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he's\": \"he is\",\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\"you'll\": \"you will\",\"you're\": \"you are\"\n",
        "}\n",
        "\n",
        "def preprocess_english(text):\n",
        "    '''Given a text this function removes the punctuations and returns the remaining text string'''\n",
        "    new_text = \"\"\n",
        "    text = text.lower()\n",
        "    i = 0\n",
        "    for word in text.split():\n",
        "      if i==0:\n",
        "        new_text = contractions.get(word,word)\n",
        "      else:\n",
        "        new_text = new_text + \" \" + contractions.get(word,word)\n",
        "      i += 1\n",
        "    return new_text.replace(\"'s\", '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "197JPEQkCGin"
      },
      "outputs": [],
      "source": [
        "X_train['multiple_choice_answer'] = X_train['multiple_choice_answer'].apply(lambda x: preprocess_english(x))\n",
        "X_val['multiple_choice_answer'] = X_val['multiple_choice_answer'].apply(lambda x: preprocess_english(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4h0wESLCLqc"
      },
      "outputs": [],
      "source": [
        "all_classes = X_train['multiple_choice_answer'].values\n",
        "class_frequency = {}\n",
        "\n",
        "for _cls in all_classes:\n",
        "    if(class_frequency.get(_cls,-1)>0):\n",
        "        class_frequency[_cls] += 1\n",
        "    else:\n",
        "        class_frequency[_cls] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fMU2WaLBk4y"
      },
      "outputs": [],
      "source": [
        "sort_class_frequency = sorted(list(class_frequency.items()),key = lambda x: x[1],reverse=True)   \n",
        "\n",
        "plt.plot([x[1] for x in sort_class_frequency[:30]])\n",
        "i=np.arange(30)\n",
        "plt.title(\"Frequency of top 30 Classes\")\n",
        "plt.xlabel(\"Tags\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.xticks(i,[x[0] for x in sort_class_frequency[:30]])\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG-upPLnCTCT"
      },
      "outputs": [],
      "source": [
        "def getPercentageOfDataCoversGivenNumClasses(n_class, class_frequency, df):\n",
        "    n_common_class = heapq.nlargest(n_class, class_frequency, key=class_frequency.get)\n",
        "    count = 0\n",
        "    for _class in df['multiple_choice_answer'].values:\n",
        "        if(_class in n_common_class):\n",
        "            count += 1\n",
        "\n",
        "    return (count/len(df))*100\n",
        "    \n",
        "n_classes = [5,10,20,50,70,100,200,500,750,1000,2000,3000,5000]\n",
        "percentage_cover = []\n",
        "for i in n_classes:\n",
        "    temp = getPercentageOfDataCoversGivenNumClasses(i,class_frequency,X_train)\n",
        "    percentage_cover.append(temp)\n",
        "    print(\"{} most frequent Classes covers {:.2f}% points\".format(i,temp))\n",
        "\n",
        "\n",
        "plt.plot(percentage_cover)\n",
        "i=np.arange(len(percentage_cover))\n",
        "plt.title(\"Percentage of points covers vs number of Classes\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.xticks(i,n_classes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhPmat6zCYFH"
      },
      "outputs": [],
      "source": [
        "common_tags = heapq.nlargest(1000, class_frequency, key = class_frequency.get)\n",
        "X_train['multiple_choice_answer'] =  X_train['multiple_choice_answer'].apply(lambda x: x if x in common_tags else '')\n",
        "\n",
        "#removing question which has empty tags\n",
        "X_train = X_train[X_train['multiple_choice_answer'].apply(lambda x: len(x)>0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRUn2o-RCf_M"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelBinarizer()\n",
        "answer_vector_train = label_encoder.fit_transform(X_train['multiple_choice_answer'].apply(lambda x: x).values)\n",
        "answer_vector_val = label_encoder.transform(X_val['multiple_choice_answer'].apply(lambda x: x).values)\n",
        "\n",
        "ans_vocab = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
        "\n",
        "print(\"Number of clasess: \", len(ans_vocab))\n",
        "print(\"Shape of Answer Vectors in Train Data: \", answer_vector_train.shape)\n",
        "print(\"Shape of Answer Vectors in Validation Data: \", answer_vector_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le61vI-rCm1j"
      },
      "source": [
        "**Creating Image features using VGG19**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HmYAhcsrvtcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tSdCehdvuF_"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import keras\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras.layers as layers\n",
        "import math\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras import layers\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    print(\"img after channels shape\")\n",
        "    print(img.shape)\n",
        "    img = tf.image.resize(img, (img_width, img_height))\n",
        "    print(\"img after resize shape\")\n",
        "    print(img.shape)\n",
        "    \n",
        "    #img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "    img = img * (1./255)\n",
        "    return img, image_path\n",
        "\n",
        "\n",
        "def Alex_Top():\n",
        "  print(\"Toy ResNet model for CIFAR10\")\n",
        "  print(\"Layers generated for model\")\n",
        "  inputs = keras.Input(shape=(224, 224, 3), name=\"img\")\n",
        "  x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
        "  block_1_output = layers.MaxPooling2D(3)(x)\n",
        "\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "  block_2_output = layers.add([x, block_1_output])\n",
        "\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "  block_3_output = layers.add([x, block_2_output])\n",
        "\n",
        "  x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
        "  x = layers.GlobalAveragePooling2D()(x)\n",
        "  x = layers.Dense(256, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "\n",
        "  model = keras.Model(inputs, outputs, name=\"toy_resnet\")\n",
        "  print(\"More information about the model\")\n",
        "  model.summary()\n",
        "  return model\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def generateImageFeatures(images):\n",
        "    model = Alex_Top()\n",
        "    all_image_dict = {}\n",
        "  \n",
        "    img_ds = tf.data.Dataset.from_tensor_slices(images)\n",
        "    img_ds = img_ds.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "    \n",
        "    for batch_img, batch_path in img_ds:\n",
        "        batch_img_features = model(batch_img)\n",
        "        #print(batch_img_features)\n",
        "\n",
        "        for img_features, path in zip(batch_img_features, batch_path):\n",
        "            image_path = path.numpy().decode(\"utf-8\")\n",
        "            print(image_path)\n",
        "            \n",
        "            \n",
        "            image_path = image_path.replace(imageDirectory,'/content/drive/MyDrive/newresults/data'+'/'+'train2014_Numpy').replace('.jpg',\"\")\n",
        "            print(image_path)\n",
        "            np.save(image_path, img_features.numpy())\n",
        "            all_image_dict[image_path] = img_features.numpy()\n",
        "            print(\"Printing image features\")\n",
        "            #print(all_image_dict[image_path])\n",
        "    \n",
        "    with open('filenamert.pickle', 'wb') as handle:\n",
        "        pickle.dump(all_image_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "    return\n",
        "\n",
        "!pwd\n",
        "all_image_path = data['image_id'].apply(lambda x:  imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (x)).unique()\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "generateImageFeatures(all_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''model = keras.Sequential()\n",
        "    model.add(layers.Conv2D(filters=96, kernel_size=(11, 11), \n",
        "                        strides=(4, 4), activation=\"relu\", \n",
        "                        input_shape=(227, 227, 3)))\n",
        "\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(pool_size=(3, 3), strides= (2, 2)))\n",
        "    model.add(layers.Conv2D(filters=256, kernel_size=(5, 5), \n",
        "                        strides=(1, 1), activation=\"relu\", \n",
        "                        padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "    model.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n",
        "                        strides=(1, 1), activation=\"relu\", \n",
        "                        padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(filters=384, kernel_size=(3, 3), \n",
        "                        strides=(1, 1), activation=\"relu\", \n",
        "                        padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), \n",
        "                        strides=(1, 1), activation=\"relu\", \n",
        "                        padding=\"same\"))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(86, activation=\"relu\"))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    \n",
        "    return model'''"
      ],
      "metadata": {
        "id": "J-jEWrrgxvUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KBOh8QMcvlN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6dE_YmJUIL1"
      },
      "outputs": [],
      "source": [
        "print(imageDirectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8xyP93SERc9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRfPkoymC6x3"
      },
      "outputs": [],
      "source": [
        "image_paths_train = X_train['image_id'].apply(lambda x:  imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (x)).values\n",
        "image_paths_val = X_val['image_id'].apply(lambda x:  imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (x)).values\n",
        "#print(image_paths_train)\n",
        "with open('filenamert.pickle','rb') as handle:\n",
        "    all_image_dict = pickle.load(handle)\n",
        "\n",
        "#with open('all_image_dict_new.pickle', 'rb') as handle:\n",
        "    #all_image_dict_new = pickle.load(handle)\n",
        "\n",
        "all_image_dict.update(all_image_dict)\n",
        "#del all_image_dict_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAOTn-UCKw-J"
      },
      "outputs": [],
      "source": [
        "print(type(image_paths_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmT0iP2QDBDh"
      },
      "source": [
        "**Creating question vectors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYs1V8g7C-Li"
      },
      "outputs": [],
      "source": [
        "def preprocess_english(text):\n",
        "    '''Given a text this function removes the punctuations and returns the remaining text string'''\n",
        "    new_text = \"<start>\"\n",
        "    text = text.lower()\n",
        "    for word in text.split():\n",
        "      new_text = new_text + \" \" + contractions.get(word,word)\n",
        "    new_text = new_text + \" <end>\"\n",
        "    return new_text.replace(\"'s\", '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQdV4vyUDIJO"
      },
      "outputs": [],
      "source": [
        "X_train['question'] = X_train['question'].apply(lambda x: preprocess_english(x))\n",
        "X_val['question'] = X_val['question'].apply(lambda x: preprocess_english(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHipg-x-DLtN"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token = \"<unk>\", filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(X_train['question'].values)\n",
        "train_question_seqs = tokenizer.texts_to_sequences(X_train['question'].values)\n",
        "val_question_seqs = tokenizer.texts_to_sequences(X_val['question'].values)\n",
        "\n",
        "print(\"Number of words in tokenizer:\", len(tokenizer.word_index))\n",
        "ques_vocab = tokenizer.word_index\n",
        "\n",
        "#Padding\n",
        "#tokenizer.word_index['<pad>'] = 0\n",
        "#tokenizer.index_word[0] = '<pad>'\n",
        "question_vector_train = tf.keras.preprocessing.sequence.pad_sequences(train_question_seqs, padding='post')\n",
        "question_vector_val = tf.keras.preprocessing.sequence.pad_sequences(val_question_seqs,padding='post',maxlen=question_vector_train.shape[1])\n",
        "\n",
        "print(\"Shape of Question Vectors in Train Data: \", question_vector_train.shape)\n",
        "print(\"Shape of Question Vectors in Validation Data: \", question_vector_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th7OexILDSwW"
      },
      "source": [
        "**Creating dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y60yQKRKDPv9"
      },
      "outputs": [],
      "source": [
        "def get_imageTensor(img, ques):\n",
        "    path = img.decode('utf-8').replace(imageDirectory,'/content/drive/MyDrive/newresults/data'+'/'+'train2014_Numpy').replace('.jpg',\"\") +'.npy'\n",
        "    \n",
        "    img_tensor = np.load(path)\n",
        "    #img_tensor = all_image_dict[img.decode('utf-8')]\n",
        "    \n",
        "    return img_tensor, ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pfa79_QaDZEW"
      },
      "outputs": [],
      "source": [
        "def createDataset(image_paths,question_vector,answer_vector):\n",
        "    dataset_input = tf.data.Dataset.from_tensor_slices((image_paths, question_vector.astype(np.float32)))\n",
        "    dataset_output = tf.data.Dataset.from_tensor_slices((answer_vector.astype(np.float32)))\n",
        "    # using map to load the numpy files in parallel\n",
        "    dataset_input = dataset_input.map(lambda img, ques : tf.numpy_function(get_imageTensor, [img, ques], [tf.float32, tf.float32]),\n",
        "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # shuffling and batching\n",
        "    #dataset_input = dataset_input.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    dataset_input = dataset_input.batch(BATCH_SIZE)\n",
        "    \n",
        "    dataset_output = dataset_output.batch(BATCH_SIZE)#.repeat()\n",
        "    \n",
        "    dataset = tf.data.Dataset.zip((dataset_input, dataset_output))\n",
        "    dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybsTYqm8Demg"
      },
      "outputs": [],
      "source": [
        "dataset_train = createDataset(image_paths_train, question_vector_train, answer_vector_train)\n",
        "dataset_val = createDataset(image_paths_val, question_vector_val, answer_vector_val)\n",
        "print(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TrtKzjtRXDV"
      },
      "outputs": [],
      "source": [
        "print(type(dataset_train))\n",
        "print(dataset_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byxoOdYlsleC"
      },
      "outputs": [],
      "source": [
        "hdf5_dict = {\n",
        "    \n",
        "    \"train_images\": image_paths_train,\n",
        "    \"train_ques\": question_vector_train.astype(np.float32),\n",
        "    \"train_ans\": answer_vector_train.astype(np.float32)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4THOYbdOE-1X"
      },
      "outputs": [],
      "source": [
        "print(type(dataset_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740wpm88oJo1"
      },
      "outputs": [],
      "source": [
        "!pip install h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scpSfiGCoMmH"
      },
      "outputs": [],
      "source": [
        "import h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvK9UTBDqlb5"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJCHzubviR-m"
      },
      "outputs": [],
      "source": [
        "# create HDF5 file\n",
        "with h5py.File('myhdfff5.hdf5', 'w') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    dt = h5py.special_dtype(vlen = np.dtype('float64'))\n",
        "    dset_x_imgtrain = hf.create_dataset('imgg_train', data=image_paths_train)\n",
        "    print(dset_x_imgtrain)\n",
        "    dset_x_questrain = hf.create_dataset('quess_train', data=question_vector_train)\n",
        "    dset_x_anstrain = hf.create_dataset('anss_train', data=answer_vector_train)\n",
        "    '''for each in dset_x_anstrain:\n",
        "      print(each)'''\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QSyRzTHLWpo"
      },
      "outputs": [],
      "source": [
        "# create HDF5 file\n",
        "with h5py.File('training.hdf5', 'w') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    dt = h5py.special_dtype(vlen = np.dtype('float64'))\n",
        "    dset_x_imgtrain = hf.create_dataset('imgg_train', data=image_paths_train)\n",
        "    print(dset_x_imgtrain)\n",
        "    dset_x_questrain = hf.create_dataset('quess_train', data=question_vector_train)\n",
        "    dset_x_anstrain = hf.create_dataset('anss_train', data=answer_vector_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qR-RU40o-du"
      },
      "outputs": [],
      "source": [
        "# create HDF5 file\n",
        "with h5py.File('training.hdf5', 'r') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    img_data = hf.get('imgg_train')\n",
        "    img_data1 = np.array(img_data)\n",
        "    print(img_data1.shape)\n",
        "    ques_data = hf.get('quess_train')\n",
        "    ques_data1 = np.array(ques_data)\n",
        "    print(ques_data1.shape)\n",
        "    anss_data = hf.get('anss_train')\n",
        "    anss_data1 = np.array(anss_data)\n",
        "    print(anss_data1.shape)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFwCutdXMgJR"
      },
      "outputs": [],
      "source": [
        "# create HDF5 file\n",
        "with h5py.File('validation.hdf5', 'w') as hf:\n",
        "    ls = list(hf.keys())\n",
        "    print('list of dataset in this file: \\n',ls)\n",
        "    dt = h5py.special_dtype(vlen = np.dtype('float64'))\n",
        "    dset_x_imgval = hf.create_dataset('imgg_val', data=image_paths_val)\n",
        "    print(dset_x_imgtrain)\n",
        "    dset_x_quesval = hf.create_dataset('quess_val', data=question_vector_val)\n",
        "    dset_x_ansval = hf.create_dataset('anss_val', data=answer_vector_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrZXbCFGBby6"
      },
      "outputs": [],
      "source": [
        "print(dset_x_anstrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7kaSkvDj0X"
      },
      "source": [
        "**Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZX3EGGaDg9U"
      },
      "outputs": [],
      "source": [
        "##fixing numpy RS\n",
        "np.random.seed(42)\n",
        "\n",
        "##fixing tensorflow RS\n",
        "tf.random.set_seed(32)\n",
        "\n",
        "##python RS\n",
        "rn.seed(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-pM8mmbDqOq"
      },
      "outputs": [],
      "source": [
        "'''def callBacksList():\n",
        "    \"\"\"\n",
        "    returns list of callback's\n",
        "    \"\"\"\n",
        "    filepath = modelsDirectory + ModelName + \"/best.hdf5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'auto')\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', patience = 3)\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 4, verbose = 1)\n",
        "\n",
        "    #directory for tensorboard to save evnts\n",
        "    log_dir= modelsDirectory + \"logs/fit/\" + ModelName + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "    print(\"TensorBoard Folder for this Execution\",log_dir)#creating TensorBoard call back,this will write all events to given folder\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
        "\n",
        "    history = tf.keras.callbacks.History()\n",
        "    callbacks_list = [reduce_lr, early_stop, history, tensorboard_callback, checkpoint]\n",
        "    return callbacks_list\n",
        "\n",
        "def Build_BaseModel():\n",
        "    image_input = tf.keras.layers.Input(shape=(7,7,512))\n",
        "    question_input = tf.keras.layers.Input(shape=(question_vector_train.shape[1],))\n",
        "\n",
        "    image_conv_layer1 = tf.keras.layers.Conv2D(filters = 4096, kernel_size = 7 , strides = 1, padding = \"valid\", activation = 'relu',\n",
        "                                               kernel_initializer = tf.keras.initializers.he_normal(seed=45))(image_input)\n",
        "\n",
        "    image_flatten = tf.keras.layers.Flatten()(image_conv_layer1)\n",
        "\n",
        "    image_dense_1 = tf.keras.layers.Dense(4096, activation = tf.nn.relu, \n",
        "                                          kernel_initializer = tf.keras.initializers.he_uniform(seed=54))(image_flatten)\n",
        "    \n",
        "    image_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, \n",
        "                                          kernel_initializer = tf.keras.initializers.he_uniform(seed=32))(image_dense_1)\n",
        "\n",
        "\n",
        "    # Input 2 Pathway\n",
        "    question_emb = tf.keras.layers.Embedding(input_dim = len(tokenizer.word_index) + 1, output_dim = 300 ,name = \"Embedding_Layer\",\n",
        "                                             embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23))(question_input)\n",
        "\n",
        "    question_lstm = tf.keras.layers.LSTM(1024, \n",
        "                                         kernel_initializer = tf.keras.initializers.glorot_uniform(seed=26),\n",
        "                                         recurrent_initializer = tf.keras.initializers.orthogonal(seed=54),\n",
        "                                         bias_initializer=tf.keras.initializers.zeros())(question_emb)\n",
        "\n",
        "    question_flatten = tf.keras.layers.Flatten(name=\"Flatten_lstm\")(question_lstm)\n",
        "\n",
        "    \n",
        "    image_question = tf.keras.layers.Multiply()([image_dense_2, question_flatten])\n",
        "\n",
        "\n",
        "    image_question_dense_1 = tf.keras.layers.Dense(1000, activation = tf.nn.relu,\n",
        "                                                    kernel_initializer = tf.keras.initializers.he_uniform(seed=19))(image_question)\n",
        "    \n",
        "    image_question_dense_2 = tf.keras.layers.Dense(1000, activation = tf.nn.relu, \n",
        "                                                   kernel_initializer = tf.keras.initializers.he_uniform(seed=28))(image_question_dense_1)\n",
        "\n",
        "    output = tf.keras.layers.Dense(len(ans_vocab), activation=tf.nn.softmax, \n",
        "                                   kernel_initializer = tf.keras.initializers.glorot_normal(seed=19))(image_question_dense_2)\n",
        "\n",
        "    # Create Model\n",
        "    model = tf.keras.models.Model(inputs = [image_input, question_input], outputs = output)\n",
        "    # Compile\n",
        "    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return model'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWu5p6NLdHfG"
      },
      "outputs": [],
      "source": [
        "def callBacksList():\n",
        "    \"\"\"\n",
        "    returns list of callback's\n",
        "    \"\"\"\n",
        "    filepath = modelsDirectory + ModelName + \"/best.hdf5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = filepath, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'auto')\n",
        "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy', patience = 3)\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience = 4, verbose = 1)\n",
        "\n",
        "    #directory for tensorboard to save evnts\n",
        "    log_dir= modelsDirectory + \"logs/fit/\" + ModelName + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "    print(\"TensorBoard Folder for this Execution\",log_dir)#creating TensorBoard call back,this will write all events to given folder\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1)\n",
        "\n",
        "    history = tf.keras.callbacks.History()\n",
        "    callbacks_list = [reduce_lr, early_stop, history, tensorboard_callback, checkpoint]\n",
        "    return callbacks_list\n",
        "\n",
        "def Build_BaseModel():\n",
        "    #image_input = tf.keras.layers.Input(shape=(7,7,512))\n",
        "    image_input = tf.keras.layers.Input(shape=(10,))\n",
        "    print(\"image input shape\")\n",
        "    print(image_input.shape)\n",
        "    question_input = tf.keras.layers.Input(shape=(question_vector_train.shape[0],))\n",
        "    print(\"question input shape\")\n",
        "    print(question_input.shape)\n",
        "    \n",
        "\n",
        "    #image_conv_layer1 = tf.keras.layers.Conv2D(filters = 4096, kernel_size = 7 , strides = 1, padding = \"valid\",input_shape='image_input', activation = 'relu',\n",
        "                                              #kernel_initializer = tf.keras.initializers.he_normal(seed=45))(image_input)\n",
        "\n",
        "    #image_flatten = tf.keras.layers.Flatten()(image_conv_layer1)\n",
        "\n",
        "    image_dense_1 = tf.keras.layers.Dense(64, activation = tf.nn.relu, \n",
        "                                          kernel_initializer = tf.keras.initializers.he_uniform(seed=54))(image_input)\n",
        "                                         \n",
        "    print(\"image dense 1 shape\")\n",
        "    print(image_dense_1.shape)                                      \n",
        "    \n",
        "    #image_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, \n",
        "                                          #kernel_initializer = tf.keras.initializers.he_uniform(seed=32))(image_dense_1)\n",
        "    img_res = tf.reshape(image_dense_1,shape=[-1,4096]) \n",
        "    print(\"image dense 2 shape\")\n",
        "    #print(image_dense_2.shape)\n",
        "\n",
        "    # Input 2 Pathway\n",
        "    question_emb = tf.keras.layers.Embedding(input_dim = len(tokenizer.word_index) + 1, output_dim = 300 ,name = \"Embedding_Layer\",\n",
        "                                             embeddings_initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23))(question_input)\n",
        "    print(\"question emb shape\")\n",
        "    print(question_emb.shape)                                         \n",
        "\n",
        "    question_lstm = tf.keras.layers.LSTM(1024, \n",
        "                                         kernel_initializer = tf.keras.initializers.glorot_uniform(seed=26),\n",
        "                                         recurrent_initializer = tf.keras.initializers.orthogonal(seed=54),\n",
        "                                         bias_initializer=tf.keras.initializers.zeros())(question_emb)\n",
        "                                         \n",
        "    print(\"question lstm shape\")\n",
        "    print(question_lstm.shape)\n",
        "    question_flatten = tf.keras.layers.Flatten(name=\"Flatten_lstm\")(question_lstm)\n",
        "    \n",
        "    print(\"question_flatten shape\")\n",
        "    print(question_flatten.shape)\n",
        "    ques_res = tf.reshape(question_flatten,shape=[-1,4096]) \n",
        "    #image_question = tf.keras.layers.Multiply()([img_res, ques_res],1)\n",
        "    image_question = tf.concat([image_dense_1,question_flatten],axis=1)\n",
        "    print(\"image_question shape\")\n",
        "    print(image_question.shape)\n",
        "\n",
        "    #image_question_dense_1 = tf.keras.layers.Dense(1024, activation = tf.nn.relu,\n",
        "                                                   # kernel_initializer = tf.keras.initializers.he_uniform(seed=19))(image_question)\n",
        "    print(\"image question shape\")\n",
        "    #print(image_question_dense_1.shape) \n",
        "    #image_question_flat = tf.reshape(image_question,shape=[-1,4096]) \n",
        "    '''print(\"image_question_flat shape\")\n",
        "    print(image_question_flat.shape)'''                                             \n",
        "    \n",
        "    image_question_dense_2 = tf.keras.layers.Dense(1024, activation = tf.nn.relu, \n",
        "                                                   kernel_initializer = tf.keras.initializers.he_uniform(seed=28))(image_question)\n",
        "\n",
        "    print(\"image_question_dense_2.shape\") \n",
        "    print(image_question_dense_2.shape)                                               \n",
        "\n",
        "    output = tf.keras.layers.Dense(len(ans_vocab), activation=tf.nn.softmax, \n",
        "                                   kernel_initializer = tf.keras.initializers.glorot_normal(seed=19))(image_question_dense_2)\n",
        "    print(\"output shape\")\n",
        "    print(output[0].shape)\n",
        "\n",
        "    # Create Model\n",
        "    modelc = tf.keras.models.Model(inputs = [image_input, question_input], outputs = output)\n",
        "   # modelc = tf.keras.models.Model(inputs = [image_input, question_input], logits = output)\n",
        "  \n",
        "    # Compile\n",
        "    modelc.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "    return modelc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNGYgptlymqj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrKZl4VADwIG"
      },
      "outputs": [],
      "source": [
        "\n",
        "l2_alpha = 0.001\n",
        "ModelName = \"BaselineModel\"\n",
        "modelb = Build_BaseModel()\n",
        "modelb.summary()\n",
        "model_vqa = modelb.save('VQAs_MODEL_WEIGHTS.hdf5')\n",
        "mode_save = modelb.save('my_vqas_model')\n",
        "\n",
        "#model.fit(train_ddata, epochs = 1, callbacks = callBacksList())\n",
        "#model.fit(train_ddata, epochs = 1, validation_data = dataset_val, callbacks = callBacksList())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIRdbWEeMtrd"
      },
      "outputs": [],
      "source": [
        "#modelb.fit(dataset_train, epochs = 20, callbacks = callBacksList())\n",
        "modelb.fit(dataset_train, epochs = 20, validation_data = dataset_val, callbacks = callBacksList())\n",
        "#model.fit(dataset_train, epochs = 2, validation_data = dataset_val, callbacks = callBacksList())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdCqceN0dZRY"
      },
      "outputs": [],
      "source": [
        "recons_model = tf.keras.models.load_model('/content/drive/MyDrive/newresults/my_vqas_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU4gEyTqD0T2"
      },
      "outputs": [],
      "source": [
        "recons_model.load_weights('/content/drive/MyDrive/newresults/VQAs_MODEL_WEIGHTS.hdf5')\n",
        "#recons_model.evaluate(dataset_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUWvOOPvD4uf"
      },
      "outputs": [],
      "source": [
        "'''%load_ext tensorboard\n",
        "%tensorboard --logdir Models/logs/fit/BaselineModel'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9hDNC9SD9va"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtloHxJ-D8Zd"
      },
      "outputs": [],
      "source": [
        "#model.load_weights(modelsDirectory + ModelName + \"/best.hdf5\")\n",
        "test_idx = np.random.randint(len(X_val), size = 3)\n",
        "model_vgg = Alex_Top()\n",
        "k = 5\n",
        "\n",
        "for idx in test_idx:\n",
        "    test_image_id = X_val['image_id'].values[idx]\n",
        "    test_question = X_val['question'].values[idx]\n",
        "    actual_answer = X_val['multiple_choice_answer'].values[idx]\n",
        "    test_image_path = imageDirectory + 'COCO_train2014_' + '%012d.jpg' % (test_image_id)\n",
        "\n",
        "    test_image_features = model_vgg(tf.expand_dims(load_image(test_image_path)[0], 0))\n",
        "    test_question_features = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([test_question]),padding='post',\n",
        "                                                                           maxlen=question_vector_train.shape[0])\n",
        "    y_pred = recons_model.predict([test_image_features,test_question_features])\n",
        "\n",
        "    class_indices = tf.math.top_k(y_pred,k=k).indices.numpy()\n",
        "    percentages = tf.math.top_k(y_pred,k=k).values.numpy()[0] * 100\n",
        "    predictions = []\n",
        "    for idx,i in enumerate(class_indices[0]):\n",
        "        classes = np.zeros((1,1000))\n",
        "        classes[0][i] = 1\n",
        "        predictions.append((label_encoder.inverse_transform(classes)[0],percentages[idx]))\n",
        "\n",
        "    img=mpimg.imread(test_image_path)\n",
        "    imgplot = plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Question :\", test_question.replace(\"<start> \",\"\").replace(\" <end>\",\"\"))\n",
        "    print(\"Actual Answer: \", actual_answer)\n",
        "    print(\"Top Predicted answers: \",predictions)\n",
        "    print(\"*\"*150)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": " replacing_vgg_with_RESnetnovcode_batch_size64",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}